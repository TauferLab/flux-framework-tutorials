{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1be3aa2",
   "metadata": {},
   "source": [
    "The code and examples that this tutorial is based on can be found here: https://github.com/flux-framework/flux-workflow-examples.git\n",
    "\n",
    "# Creating Flux Instances\n",
    "\n",
    "Flux includes a sub-command for bootstrapping: `flux start`.  On an HPC system, you would use this in the same way as an MPI application; for example on a Slurm cluster, you would run `srun flux start`.  For local development and testing purposes, you can start multiple broker ranks on a single node by passing the `--size=N` flag to `flux start`.  For example, to start a Flux session with 4 brokers on the local node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "476a9ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\r\n"
     ]
    }
   ],
   "source": [
    "!flux start --size=4 flux getattr size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd7d08",
   "metadata": {},
   "source": [
    "# Submitting Jobs to Flux\n",
    "## Submission CLI\n",
    "\n",
    "To submit jobs to Flux, you can use the `flux mini` command, which has several sub-commands: `submit`, `run`, `bulksubmit`, `batch`, and `alloc`.  The `flux mini submit` command submits a job to Flux and prints out the jobid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b922ed40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒASYbkbZ\r\n"
     ]
    }
   ],
   "source": [
    "!flux mini submit hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac798095",
   "metadata": {},
   "source": [
    "The `flux mini run` command submit a job to Flux (similar to `flux mini submit`) but then it attaches to the job (with `flux job attach`), printing the job's stdout/stderr to the terminal and exiting with the same exit code as the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d546c3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flux-job: task(s) exited with exit code 1\r\n"
     ]
    }
   ],
   "source": [
    "!flux mini run /bin/false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5213d",
   "metadata": {},
   "source": [
    "`flux mini submit` and `flux mini run` also support many other useful flags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02032748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 1edf4b264187\n",
      "3: 1edf4b264187\n",
      "0: 1edf4b264187\n",
      "2: 1edf4b264187\n",
      "usage: flux-mini run [-h] [-t FSD] [--urgency N] [--job-name NAME] [-o OPT]\n",
      "                     [--setattr ATTR=VAL] [--env RULE] [--env-remove PATTERN]\n",
      "                     [--env-file FILE] [--input FILENAME] [--output FILENAME]\n",
      "                     [--error FILENAME] [-l] [--flags FLAGS] [--dry-run]\n",
      "                     [-N N] [-n N] [-c N] [-g N] [-v]\n",
      "                     ...\n",
      "\n",
      "positional arguments:\n",
      "  command                   Job command and arguments\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help                show this help message and exit\n",
      "  -t, --time-limit=FSD      Time limit in Flux standard duration, e.g. 2d,\n",
      "                            1.5h\n",
      "      --urgency=N           Set job urgency (0-31), hold=0, default=16,\n",
      "                            expedite=31\n",
      "      --job-name=NAME       Set an optional name for job to NAME\n",
      "  -o, --setopt=OPT          Set shell option OPT. An optional value is\n",
      "                            supported with OPT=VAL (default VAL=1) (multiple\n",
      "                            use OK)\n",
      "      --setattr=ATTR=VAL    Set job attribute ATTR to VAL (multiple use OK)\n",
      "      --env=RULE            Control how environment variables are exported. If\n",
      "                            RULE starts with '-' apply rest of RULE as a\n",
      "                            remove filter (see --env-remove), if '^' then read\n",
      "                            rules from a file (see --env-file). Otherwise, set\n",
      "                            matching environment variables from the current\n",
      "                            environment (--env=PATTERN) or set a value\n",
      "                            explicitly (--env=VAR=VALUE). Rules are applied in\n",
      "                            the order they are used on the command line.\n",
      "                            (multiple use OK)\n",
      "      --env-remove=PATTERN  Remove environment variables matching PATTERN. If\n",
      "                            PATTERN starts with a '/', then it is matched as a\n",
      "                            regular expression, otherwise PATTERN is a shell\n",
      "                            glob expression. (multiple use OK)\n",
      "      --env-file=FILE       Read a set of environment rules from FILE.\n",
      "                            (multiple use OK)\n",
      "      --input=FILENAME      Redirect job stdin from FILENAME, bypassing KVS\n",
      "      --output=FILENAME     Redirect job stdout to FILENAME, bypassing KVS\n",
      "      --error=FILENAME      Redirect job stderr to FILENAME, bypassing KVS\n",
      "  -l, --label-io            Add rank labels to stdout, stderr lines\n",
      "      --flags=FLAGS         Set comma separated list of job submission flags.\n",
      "                            Possible flags: debug, waitable\n",
      "      --dry-run             Don't actually submit job, just emit jobspec\n",
      "  -N, --nodes=N             Number of nodes to allocate\n",
      "  -n, --ntasks=N            Number of tasks to start\n",
      "  -c, --cores-per-task=N    Number of cores to allocate per task\n",
      "  -g, --gpus-per-task=N     Number of GPUs to allocate per task\n",
      "  -v, --verbose             Increase verbosity on stderr (multiple use OK)\n"
     ]
    }
   ],
   "source": [
    "!flux mini run -n4 --label-io --time-limit=5s --env-remove=LD_LIBRARY_PATH hostname\n",
    "!flux mini run --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e9ed6c",
   "metadata": {},
   "source": [
    "The `flux mini bulksubmit` makes submitting the same executable repeatedly very simple.  It leverages the same syntax as GNU's parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e82702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒBBk6prK\n",
      "ƒBBmap8f\n",
      "ƒBBmap8g\n",
      "foo\n",
      "baz\n",
      "bar\n"
     ]
    }
   ],
   "source": [
    "!flux mini bulksubmit --watch --wait echo {} ::: foo bar baz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e0c8c4",
   "metadata": {},
   "source": [
    "Of course, Flux can launch more than just single-node, single-core jobs.  We can submit multiple heterogeneous jobs, and Flux will co-schedule the jobs while also ensuring no oversubscription of resources (e.g., cores).\n",
    "\n",
    "Note: in this tutorial, we cannot assume that the host you are running on has multiple cores, thus the examples below only vary the number of nodes per job.  Varying the `cores-per-task` is also possible on Flux when the underlying hardware supports it (e.g., a multi-core node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "brazilian-former",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒefTJnFy\n",
      "ƒeqeqqhZ\n"
     ]
    }
   ],
   "source": [
    "!flux mini submit --nodes=2 --ntasks=2 --cores-per-task=1 --job-name simulation sleep inf\n",
    "!flux mini submit --nodes=1 --ntasks=1 --cores-per-task=1 --job-name analysis sleep inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c2af2",
   "metadata": {},
   "source": [
    "We can now list out the jobs in the queue with `flux jobs`, and we should see both jobs that we just submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "institutional-vocabulary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       JOBID USER     NAME       ST NTASKS NNODES  RUNTIME NODELIST\r\n",
      "    ƒeqeqqhZ fluxuser analysis    R      1      1   0.539s 1edf4b264187\r\n",
      "    ƒefTJnFy fluxuser simulation  R      2      2   0.935s 1edf4b[264187,264187]\r\n"
     ]
    }
   ],
   "source": [
    "!flux jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca4277",
   "metadata": {},
   "source": [
    "Since those jobs won't ever exit (and we didn't specify a timelimit), let's kill them off now and free up the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aggressive-democracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flux-job: Terminated 2 jobs (0 errors)\n",
      "       JOBID USER     NAME       ST NTASKS NNODES  RUNTIME NODELIST\n"
     ]
    }
   ],
   "source": [
    "!flux job killall -f\n",
    "!flux jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544aa0a9",
   "metadata": {},
   "source": [
    "We can use the `flux mini batch` command to easily created nested flux instances.  When `flux mini batch` is invoked, Flux will automatically create a nested instance that spans the resources allocated to the job, and then Flux runs the batch script passed to `flux mini batch` on rank 0 of the nested instance. While a batch script is expected to launch parallel jobs using `flux mini run` or `flux mini submit` at this level, nothing prevents the script from further batching other sub-batch-jobs using the `flux mini batch` interface, if desired.\n",
    "\n",
    "Note: Flux also provides a `flux mini alloc` which is an interactive version of `flux mini batch`, but demonstrating that in a Jupyter notebook is difficult due to the lack of pseudo-terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "blank-carpet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒ2zxhErij\n",
      "ƒ318V47eo\n"
     ]
    }
   ],
   "source": [
    "!flux mini batch --nslots=2 --cores-per-slot=1 --nodes=2 ./sleep_batch.sh\n",
    "!flux mini batch --nslots=2 --cores-per-slot=1 --nodes=2 ./sleep_batch.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da98bfa1",
   "metadata": {},
   "source": [
    "The contents of `sleep_batch.sh`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-witch",
   "metadata": {},
   "source": [
    "``` bash \n",
    "    #!/bin/bash\n",
    "  \n",
    "    echo \"Starting my batch job\"\n",
    "    echo \"Print the resources allocated to this batch job\"\n",
    "    flux resource list\n",
    "\n",
    "    echo \"Use sleep to emulate a parallel program\"\n",
    "    echo \"Run the program at a total of 2 processes each requiring\"\n",
    "    echo \"1 core. These processes are equally spread across 2 nodes.\"\n",
    "    flux mini run -N 2 -n 2 sleep 10\n",
    "    flux mini run -N 2 -n 2 sleep 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "empty-cemetery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       JOBID USER     NAME       ST NTASKS NNODES  RUNTIME NODELIST\n",
      "   ƒ318V47eo fluxuser sleep_batc  R      2      2   48.45s 1edf4b[264187,264187]\n",
      "   ƒ2zxhErij fluxuser sleep_batc  R      2      2   48.83s 1edf4b[264187,264187]\n",
      "{\"version\": 1, \"execution\": {\"R_lite\": [{\"rank\": \"0-1\", \"children\": {\"core\": \"5\"}}], \"nodelist\": [\"1edf4b[264187,264187]\"], \"starttime\": 1618379436, \"expiration\": 1618984236}}\n",
      "\n",
      "0: stdout redirected to flux-ƒ318V47eo.out\n",
      "0: stderr redirected to flux-ƒ318V47eo.out\n",
      "Starting my batch job\n",
      "Print the resources allocated to this batch job\n",
      "     STATE NNODES   NCORES    NGPUS NODELIST\n",
      "      free      2        2        0 1edf4b[264187,264187]\n",
      " allocated      0        0        0 \n",
      "      down      0        0        0 \n",
      "Use sleep to emulate a parallel program\n",
      "Run the program at a total of 2 processes each requiring\n",
      "1 core. These processes are equally spread across 2 nodes.\n"
     ]
    }
   ],
   "source": [
    "!flux jobs\n",
    "\n",
    "# Copy the Job ID of one of the `flux mini batch`s here to examine the job's resources and output\n",
    "JOBID=\"ƒ318V47eo\"\n",
    "!flux job info {JOBID} R\n",
    "!flux job attach {JOBID}\n",
    "!cat flux-{JOBID}.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997faffc",
   "metadata": {},
   "source": [
    "## Submission API\n",
    "Flux also provides first-class python bindings which can be used to submit jobs programmatically. The following script shows this with the `flux.job.submit()` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "third-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import flux\n",
    "from flux.job import JobspecV1\n",
    "from flux.job.JobID import JobID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "selective-uganda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒ5Ev8fUo9\n"
     ]
    }
   ],
   "source": [
    "f = flux.Flux() # connect to the running Flux instance\n",
    "compute_jobreq = JobspecV1.from_command(\n",
    "    command=[\"./compute.py\", \"120\"], num_tasks=1, num_nodes=1, cores_per_task=1\n",
    ") # construct a jobspec\n",
    "compute_jobreq.cwd = os.path.expanduser(\"~/flux-workflow-examples/job-submit-api/\") # set the CWD\n",
    "print(JobID(flux.job.submit(f,compute_jobreq)).f58) # submit and print out the jobid (in f58 format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "logical-anderson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       JOBID USER     NAME       ST NTASKS NNODES  RUNTIME NODELIST\r\n",
      "   ƒ5Ev8fUo9 fluxuser compute.py  R      1      1   5.644s 1edf4b264187\r\n"
     ]
    }
   ],
   "source": [
    "!flux jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332f9c9",
   "metadata": {},
   "source": [
    "Under the hood, the `Jobspec` class is creating a YAML document that ultimately gets serialized as JSON and sent to Flux for ingestion, validation, queueing, scheduling, and eventually execution.  We can dump the raw JSON jobspec that is submitted, where we can see the exact resources requested and the task set to be executed on those resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efa06478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"resources\": [\n",
      "    {\n",
      "      \"type\": \"node\",\n",
      "      \"count\": 1,\n",
      "      \"with\": [\n",
      "        {\n",
      "          \"type\": \"slot\",\n",
      "          \"count\": 1,\n",
      "          \"with\": [\n",
      "            {\n",
      "              \"type\": \"core\",\n",
      "              \"count\": 1\n",
      "            }\n",
      "          ],\n",
      "          \"label\": \"task\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"tasks\": [\n",
      "    {\n",
      "      \"command\": [\n",
      "        \"./compute.py\",\n",
      "        \"120\"\n",
      "      ],\n",
      "      \"slot\": \"task\",\n",
      "      \"count\": {\n",
      "        \"per_slot\": 1\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"attributes\": {\n",
      "    \"system\": {\n",
      "      \"duration\": 0,\n",
      "      \"cwd\": \"/home/fluxuser/flux-workflow-examples/job-submit-api/\"\n",
      "    }\n",
      "  },\n",
      "  \"version\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(compute_jobreq.dumps(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bbc90e",
   "metadata": {},
   "source": [
    "We can then replicate our previous example of submitting multiple heterogeneous jobs and testing that Flux co-schedules them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "industrial-privacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒ7rvUGZYK\n",
      "ƒ7rvszN3q\n"
     ]
    }
   ],
   "source": [
    "compute_jobreq = JobspecV1.from_command(\n",
    "    command=[\"./compute.py\", \"120\"], num_tasks=4, num_nodes=2, cores_per_task=2\n",
    ")\n",
    "compute_jobreq.cwd = os.path.expanduser(\"~/flux-workflow-examples/job-submit-api/\")\n",
    "print(JobID(flux.job.submit(f, compute_jobreq)))\n",
    "\n",
    "io_jobreq = JobspecV1.from_command(\n",
    "    command=[\"./io-forwarding.py\", \"120\"], num_tasks=1, num_nodes=1, cores_per_task=1\n",
    ")\n",
    "io_jobreq.cwd = os.path.expanduser(\"~/flux-workflow-examples/job-submit-api/\")\n",
    "print(JobID(flux.job.submit(f, io_jobreq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "pregnant-creativity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       JOBID USER     NAME       ST NTASKS NNODES  RUNTIME NODELIST\r\n",
      "   ƒ7rvszN3q fluxuser io-forward  R      1      1   1.866s 1edf4b264187\r\n",
      "   ƒ7rvUGZYK fluxuser compute.py  R      4      2   1.886s 1edf4b[264187,264187]\r\n",
      "   ƒ7bXms1hZ fluxuser io-forward  R      1      1   36.81s 1edf4b264187\r\n",
      "   ƒ7bXKBEdM fluxuser compute.py  R      4      2   36.83s 1edf4b[264187,264187]\r\n"
     ]
    }
   ],
   "source": [
    "!flux jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8051640",
   "metadata": {},
   "source": [
    "We can use the FluxExecutor class to submit large numbers of jobs to Flux. This method uses python's `concurrent.futures` interface.  Example snippet from `~/flux-workflow-examples/async-bulk-job-submit/bulksubmit_executor.py`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-trace",
   "metadata": {},
   "source": [
    "``` python \n",
    "with FluxExecutor() as executor:\n",
    "        compute_jobspec = JobspecV1.from_command(args.command)\n",
    "        futures = [executor.submit(compute_jobspec) for _ in range(args.njobs)]\n",
    "        # wait for the jobid for each job, as a proxy for the job being submitted\n",
    "        for fut in futures:\n",
    "            fut.jobid()\n",
    "        # all jobs submitted - print timings\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cleared-lawsuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulksubmit_executor: submitted 200 jobs in 0.37s. 536.54job/s\n",
      "bulksubmit_executor: First job finished in about 0.582s\n",
      "|██████████████████████████████████████████████████████████| 100.0% (47.3 job/s)\n",
      "bulksubmit_executor: Ran 200 jobs in 4.2s. 47.1 job/s\n"
     ]
    }
   ],
   "source": [
    "# Submit a FluxExecutor based script.\n",
    "%run ./flux-workflow-examples/async-bulk-job-submit/bulksubmit_executor.py -n200 /bin/sleep 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec052119",
   "metadata": {},
   "source": [
    "# Diving Deeper Into Flux's Internals\n",
    "\n",
    "Flux uses hwloc to detect the resources on each node and then to populate its resource graph.  You can access the hwloc topology information that Flux collects with the `flux hwloc` subcommand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "scenic-chassis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Machines, 28 Cores, 28 PUs\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<!DOCTYPE topology SYSTEM \"hwloc.dtd\">\n",
      "<topology>\n",
      "  <object type=\"Machine\" os_index=\"0\" cpuset=\"0x0000007f\" complete_cpuset=\"0x0000007f\" online_cpuset=\"0x0000007f\" allowed_cpuset=\"0x0000007f\" nodeset=\"0x00000001\" complete_nodeset=\"0x00000001\" allowed_nodeset=\"0x00000001\">\n",
      "    <info name=\"DMIProductName\" value=\"BHYVE\"/>\n",
      "    <info name=\"DMIProductVersion\" value=\"1.0\"/>\n",
      "    <info name=\"DMIChassisVendor\" value=\"\"/>\n",
      "    <info name=\"DMIChassisType\" value=\"2\"/>\n",
      "    <info name=\"DMIChassisVersion\" value=\"1.0\"/>\n",
      "    <info name=\"DMIChassisAssetTag\" value=\"None\"/>\n"
     ]
    }
   ],
   "source": [
    "!flux hwloc info\n",
    "!flux hwloc topology | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086e47e",
   "metadata": {},
   "source": [
    "Flux can also bootstrap its resource graph based on static input files, like in the case of a multi-user system instance setup by site adminstrators.  [More information on Flux's static resource configuration files](https://flux-framework.readthedocs.io/en/latest/adminguide.html#resource-configuration).  Flux provides a more standard interface to listing available resources that works regardless of the resource input source: `flux resource`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "prime-equilibrium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    STATUS NNODES RANKS           NODELIST\n",
      "     avail      4 0-3             491ea08a8cee,491ea08a8cee,491ea08a8cee,491ea08a8cee\n",
      "     STATE NNODES   NCORES    NGPUS NODELIST\n",
      "      free      4       28        0 491ea08a8cee,491ea08a8cee,491ea08a8cee,491ea08a8cee\n",
      " allocated      0        0        0 \n",
      "      down      0        0        0 \n"
     ]
    }
   ],
   "source": [
    "# To view status of resources\n",
    "!flux resource status\n",
    "# To view scheduler's perspective on resources (allocated, free, etc)\n",
    "!flux resource list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee1c49d",
   "metadata": {},
   "source": [
    "Flux has a command for controlling the queue within the `job-manager`: `flux queue`.  This includes disabling job submission, re-enabling it, waiting for the queue to become idle or empty, and checking the queue status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "800de4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flux-queue: Job submission is disabled: maintenance outage\n",
      "flux-queue: Job submission is enabled\n",
      "Usage: flux-queue [OPTIONS] COMMAND ARGS\n",
      "  -h, --help             Display this message.\n",
      "\n",
      "Common commands from flux-queue:\n",
      "   enable          Enable job submission\n",
      "   disable         Disable job submission\n",
      "   start           Start scheduling\n",
      "   stop            Stop scheduling\n",
      "   status          Get queue status\n",
      "   drain           Wait for queue to become empty.\n",
      "   idle            Wait for queue to become idle.\n"
     ]
    }
   ],
   "source": [
    "!flux queue disable \"maintenance outage\"\n",
    "!flux queue enable\n",
    "!flux queue -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa7559",
   "metadata": {},
   "source": [
    "Each Flux instance has a set of attributes that are set at startup that affect the operation of Flux, such as `rank`, `size`, and `local-uri` (the Unix socket usable for communicating with Flux).  Many of these attributes can be modified at runtime, such as `log-stderr-level` (1 logs only critical messages to stderr while 7 logs everything, including debug messages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "biblical-generic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n",
      "local:///tmp/flux-ULJeOm/local-0\n",
      "broker.mapping                          (vector,(0,1,4))\n",
      "broker.pid                              16\n",
      "broker.quorum                           0-3\n",
      "broker.rc1_path                         /etc/flux/rc1\n",
      "broker.rc3_path                         /etc/flux/rc3\n",
      "conf.connector_path                     /usr/lib/flux/connectors\n",
      "conf.exec_path                          /usr/libexec/flux/cmd\n",
      "conf.module_path                        /usr/lib/flux/modules\n",
      "conf.pmi_library_path                   /usr/lib/flux/libpmi.so\n",
      "conf.shell_initrc                       /etc/flux/shell/initrc.lua\n",
      "conf.shell_pluginpath                   /usr/lib/flux/shell/plugins\n",
      "config.path                             -\n",
      "content.acct-dirty                      0\n",
      "content.acct-entries                    12202\n",
      "content.acct-size                       6087217\n",
      "content.acct-valid                      12202\n",
      "content.backing-module                  content-sqlite\n",
      "content.backing-path                    /tmp/flux-ULJeOm/content.sqlite\n",
      "content.blob-size-limit                 1073741824\n",
      "content.flush-batch-count               0\n",
      "content.flush-batch-limit               256\n",
      "content.hash                            sha1\n",
      "content.purge-large-entry               256\n",
      "content.purge-old-entry                 10\n",
      "content.purge-target-entries            1048576\n",
      "content.purge-target-size               16777216\n",
      "instance-level                          0\n",
      "jobid                                   -\n",
      "local-uri                               local:///tmp/flux-ULJeOm/local-0\n",
      "log-count                               736\n",
      "log-critical-level                      2\n",
      "log-filename                            -\n",
      "log-forward-level                       7\n",
      "log-level                               7\n",
      "log-ring-size                           1024\n",
      "log-ring-used                           736\n",
      "log-stderr-level                        3\n",
      "log-stderr-mode                         leader\n",
      "parent-kvs-namespace                    -\n",
      "parent-uri                              -\n",
      "rank                                    0\n",
      "rundir                                  /tmp/flux-ULJeOm\n",
      "security.owner                          1002\n",
      "size                                    4\n",
      "tbon.arity                              2\n",
      "tbon.descendants                        3\n",
      "tbon.endpoint                           ipc:///tmp/flux-ULJeOm/tbon-0\n",
      "tbon.level                              0\n",
      "tbon.maxlevel                           2\n",
      "tbon.parent-endpoint                    -\n",
      "version                                 0.24.0-87-g395b1808b\n"
     ]
    }
   ],
   "source": [
    "!flux getattr rank\n",
    "!flux getattr size\n",
    "!flux getattr local-uri\n",
    "!flux setattr log-stderr-level 3\n",
    "!flux lsattr -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74fdfcf",
   "metadata": {},
   "source": [
    "Services within a Flux instance are implemented by modules. To query and manage broker modules, use `flux module`.  Modules that we have already directly interacted with in this tutorial include `resource` (via `flux resource`), `job-ingest` (via `flux mini` and the Python API) `job-list` (via `flux jobs`) and `job-manager` (via `flux queue`), and we will interact with the `kvs` module in a few cells. For the most part, services are implemented by modules of the same name (e.g., `kvs` implements the `kvs` service and thus the `kvs.lookup` RPC).  In some circumstances, where multiple implementations for a service exist, a module of a different name implements a given service (e.g., in this instance, `sched-fluxion-qmanager` provides the `sched` service and thus `sched.alloc`, but in another instance `sched-simple` might provide the `sched` service)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "spatial-maintenance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module                       Size Digest  Idle  S Service\r\n",
      "job-ingest                1453656 AB27448 idle  R \r\n",
      "job-info                  1638128 E3A21EA idle  R \r\n",
      "job-exec                  1509296 ED8BF74 idle  R \r\n",
      "connector-local           1298368 5972E0B    0  R \r\n",
      "content-sqlite            1326664 368815B idle  R content-backing,kvs-checkpoint\r\n",
      "cron                      1407496 7D62B82 idle  R \r\n",
      "job-manager               1722016 C480039 idle  R \r\n",
      "kvs                       1835336 9E19B98 idle  R \r\n",
      "barrier                   1312256 8402DD6 idle  R \r\n",
      "heartbeat                 1291720 16D0F76    1  R \r\n",
      "resource                  1706968 B5C4125 idle  R \r\n",
      "sched-fluxion-qmanager    6374080 BEDC833 idle  R sched\r\n",
      "kvs-watch                 1528416 B7B3A25 idle  R \r\n",
      "job-list                  1710264 C3A95FB idle  R \r\n",
      "sched-fluxion-resource   31145912 80BC659 idle  R \r\n"
     ]
    }
   ],
   "source": [
    "!flux module list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7090eb",
   "metadata": {},
   "source": [
    "We can actually unload the Fluxion modules (the scheduler modules from flux-sched) and replace them with `sched-simple` (the scheduler that comes built-into flux-core) as a demonstration of this functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df4bc2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module                       Size Digest  Idle  S Service\r\n",
      "job-ingest                1453656 AB27448 idle  R \r\n",
      "job-info                  1638128 E3A21EA idle  R \r\n",
      "job-exec                  1509296 ED8BF74 idle  R \r\n",
      "connector-local           1298368 5972E0B    0  R \r\n",
      "content-sqlite            1326664 368815B idle  R content-backing,kvs-checkpoint\r\n",
      "cron                      1407496 7D62B82 idle  R \r\n",
      "job-manager               1722016 C480039    0  R \r\n",
      "kvs                       1835336 9E19B98 idle  R \r\n",
      "barrier                   1312256 8402DD6 idle  R \r\n",
      "heartbeat                 1291720 16D0F76    0  R \r\n",
      "sched-simple              1579800 0E8E6EC    0  R sched\r\n",
      "resource                  1706968 B5C4125    0  R \r\n",
      "kvs-watch                 1528416 B7B3A25 idle  R \r\n",
      "job-list                  1710264 C3A95FB idle  R \r\n"
     ]
    }
   ],
   "source": [
    "!flux module unload sched-fluxion-qmanager\n",
    "!flux module unload sched-fluxion-resource\n",
    "!flux module load sched-simple\n",
    "!flux module list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c4ecf",
   "metadata": {},
   "source": [
    "We can now reload the Fluxion scheduler, but this time, let's pass some extra arguments to specialize our Flux instance.  In particular, let's populate our resource graph with nodes, sockets, and cores and limit the scheduling depth to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c34899ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flux-module: broker.rmmod sched-simple: No such file or directory\n",
      "flux-module: broker.insmod: sched-fluxion-resource module/service is in use\n",
      "flux-module: broker.insmod: sched-fluxion-qmanager module/service is in use\n",
      "Module                       Size Digest  Idle  S Service\n",
      "sched-fluxion-qmanager    6374080 BEDC833   54  R sched\n",
      "job-ingest                1453656 AB27448 idle  R \n",
      "job-info                  1638128 E3A21EA idle  R \n",
      "job-exec                  1509296 ED8BF74 idle  R \n",
      "sched-fluxion-resource   31145912 80BC659   54  R \n",
      "connector-local           1298368 5972E0B    0  R \n",
      "content-sqlite            1326664 368815B idle  R content-backing,kvs-checkpoint\n",
      "cron                      1407496 7D62B82 idle  R \n",
      "job-manager               1722016 C480039   54  R \n",
      "kvs                       1835336 9E19B98 idle  R \n",
      "barrier                   1312256 8402DD6 idle  R \n",
      "heartbeat                 1291720 16D0F76    1  R \n",
      "resource                  1706968 B5C4125   55  R \n",
      "kvs-watch                 1528416 B7B3A25 idle  R \n",
      "job-list                  1710264 C3A95FB idle  R \n",
      "2021-04-14T06:09:25.747240Z sched-fluxion-qmanager.debug[0]: effective queue params (queue=default): queue-depth=4\n"
     ]
    }
   ],
   "source": [
    "!flux dmesg -C\n",
    "!flux module unload sched-simple\n",
    "!flux module load sched-fluxion-resource load-allowlist=node,socket,core\n",
    "!flux module load sched-fluxion-qmanager queue-params=queue-depth=4\n",
    "!flux module list\n",
    "!flux dmesg | grep queue-depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4b0e04",
   "metadata": {},
   "source": [
    "The key-value store (KVS) is a core component of a Flux instance. The `flux kvs` command provides a utility to list and manipulate values of the KVS. Modules of Flux use the KVS to persistently store information and retrieve it later on (potentially after a restart of Flux).  One example of KVS use by Flux is the `resource` module, which stores the resource set `R` of the current Flux instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "nervous-broadcast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job         resource\n",
      "R           eventlog\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"version\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"execution\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"R_lite\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
      "      \u001b[1;39m{\n",
      "        \u001b[0m\u001b[34;1m\"rank\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"0-3\"\u001b[0m\u001b[1;39m,\n",
      "        \u001b[0m\u001b[34;1m\"children\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "          \u001b[0m\u001b[34;1m\"core\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"0-6\"\u001b[0m\u001b[1;39m\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "      \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "    \u001b[1;39m]\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"starttime\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"expiration\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"nodelist\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
      "      \u001b[0;32m\"1edf4b[264187,264187,264187,264187]\"\u001b[0m\u001b[1;39m\n",
      "    \u001b[1;39m]\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!flux kvs ls \n",
    "!flux kvs ls resource\n",
    "!flux kvs get resource.R | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3920f9e",
   "metadata": {},
   "source": [
    "Flux provides a built-in mechanism for executing commands on nodes without requiring a job or resource allocation: `flux exec`.  `flux exec` is typically used by sys admins to execute administrative commands and load/unload modules across multiple ranks simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "suitable-claim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "!flux exec -r 2 flux getattr rank # only execute on rank 2\n",
    "!flux exec flux getattr rank # execute on all ranks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
